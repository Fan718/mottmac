---
title: "CCTV Stormwater Pipe Project - Logistic regression"
author: "Fan"
date: '2022-04-10'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(visdat) # visualize missing values
library(ROSE) # undersampling, oversampling and ROSE 
library(car) # Check VIF values
library(InformationValue)
library(knitr)
library(broom)
library(pscl) # Get McFadden index
library(dominanceanalysis)
library(caret)
library(survey) # importance of variables
library(plotROC)
library(fastDummies)

# Import dataset
SW <- read.csv("C:/Users/fan100199/OneDrive - Mott MacDonald/Desktop/MottMac/Data_sets\\readytogo.csv")

```

# 1. Data Prep

```{r}
# Check missing values
vis_miss(SW) # None NAs

# Structure of the dataset 
glimpse(SW)

# As the percentage of "Circular" pipe is more than 93%, 
# then we split the values into gravity (1) and others (0)
SW$pipe_shape <- ifelse(SW$pipe_shape == "CIRC", 1, 0)

# Convert material column values to numeric values
SW$material <- factor(SW$material, 
                      labels = c(1,2,3,4,5,6),
                      levels = c("OTHR", "ABCM", "CONC",
                                 "ERWR", "PYTH", "PYVN"))
# Convert operational area column values to numeric values
SW$oper_area <- factor(SW$oper_area, 
                      labels = c(1,2,3),
                      levels = c("SC","SN","SS"))

# Convert local board numbers 
# Albert-Eden(100), Devonport-Takapuna(105), Franklin(110), Henderson-Massey(120), Hibiscus and Bays(125), 
# Howick(130), Kaipatiki(135), Mangere-Otahuhu (140), Manurewa (145), Maungakiekie-Tamaki (150), 
# Orakei (155), Otara-Papatoetoe (160), Papakura (165), Puketapapa (170), Rodney (175), 
# Upper Harbour (180), Waitakere Ranges (190), Waitemata (195), Whau (200)
SW$local_board <- factor(SW$local_board,
                         labels = c(1:19),
                         levels = c(100,105,110,120,125,130,135,140,145,150,155,
                                    160,165,170,175,180,190,195,200))

# Chcek the structure again
str(SW)

# Transform two response variables to factor
SW$pipe_shape <- as.factor(SW$pipe_shape)
SW$structural_condition <- as.factor(SW$structural_condition)
SW$service_condition <- as.factor(SW$service_condition)
# Check the structure of new dataset
str(SW)
# Observe top six rows
head(SW)
```

```{r}
# Split the dataset
set.seed(222)
spl <- sample(2, nrow(SW), replace = TRUE, prob = c(0.7, 0.3))
train <- SW[spl==1,]
test <- SW[spl==2,]
rbind(dim(train), dim(test))
```

## Imbalanced datasets

The problem with imbalanced data is that the model being trained would be dominated by the majority class which would be predicted more effectively than the minority class. As we can see from the visual plots of the distribution of pipes by structural and service condition that the number of good conditional pipes are much more than that of poor conditional pipes. As a result, it would result in high value for sensitivity rate and low value for specificity rate. 

Undersampling, oversampling and hybrid methods such as ROSE have been applied to solve this problem. Undersampling works with the majority class that it reduce the number of good conditional pipes to make the data set balanced. Oversampling works with minority class and it replicated the number of poor conditional pipes to balance the data set. ROSE technique add new synthetic data points to the minority class and also reduces the majority size. 

```{r}
# Check classes proportion
table(train$structural_condition)
table(train$service_condition)

# Methods to solve imbalanced data
# 1. Down Sample the dataset due to imbalanced data in structural condition
set.seed(100)
under_train_str <- ovun.sample(structural_condition ~.,
                              data = train,
                              method = "under",
                              N = 4945*2)$data
table(under_train_str$structural_condition)
##    0    1 
##  4945 4945 

# 2. Over Sample the dataset in service condition
set.seed(100)
table(train$service_condition)
over_train_ser <- ovun.sample(service_condition ~., 
                              data = train,
                              method = "over",
                              N = 13286*2)$data
table(over_train_ser$service_condition)
##     0     1 
##  13286 13286

# 3. ROSE technique add new synthetic data points to the minority class
# and downsamples the majority class.
set.seed(111)
rose_train_str<-ROSE(structural_condition~.,data=train)$data
table(rose_train_str$structural_condition)
##   0      1 
## 7977    7999

# 4. ROSE function being used for service condition
set.seed(111)
rose_train_ser<-ROSE(service_condition~.,data=train)$data
table(rose_train_ser$service_condition)
##    0    1 
##  7977 7999 
```

# 2. Binary Logistic Models

## 2.1 Predicting Pipes in Structural Condition

### 2.1.1 Undersampling data set 
```{r}
library(MASS)
# Logistic regression models using undersampling dataset
full_str <- glm(formula = structural_condition ~ .-service_condition, 
                 family = "binomial", data = under_train_str)
summary(full_str)

# Perform backward stepwise regression
backward <- step(full_str, direction='backward', scope=formula(full_str), trace=0)
summary(backward)
# Likelihood Test
backward$anova
# check multicollinearity
car::vif(backward)
# Anova chi-square test to check the overall effect of variables
anova(backward, test = "Chisq")
# Drop columns of diameter, upstream_invert and pipe_shape

# Final model 
model_str1 <- glm(structural_condition ~ years + material + 
                    downstream_depth + pipe_length + 
                    downstream_invert + local_board,
                  family = "binomial", 
                  data = under_train_str)
summary(model_str1)

# Each predictor is statistically significant

# check the importance of each variable
varImp(model_str1)

# Model Prediction for the test dataset
pred1 <- predict(model_str1, test, type = "response")
plotROC(test$structural_condition, pred1) # 0.75

pred1 <- ifelse(pred1 > 0.5,1,0)
# Confusion matrix
confusionMatrix(as.factor(pred1), test$structural_condition, positive = "1")

```

### 2.1.2 ROSE data set 
```{r}
# ROSE dataset
full_str2 <- glm(formula = structural_condition ~ 
                   years + material + diameter +
                          pipe_length + downstream_depth +
                          upstream_depth + pipe_shape +
                          downstream_invert + upstream_invert +
                          oper_area + local_board, 
                 family = "binomial", data = rose_train_str)
summary(full_str2)

# Perform backward stepwise regression
backward2 <- step(full_str2, direction='backward', scope=formula(full_str2), trace=0)
summary(backward2)
# Likelihood Test
backward2$anova
# check multicollinearity
car::vif(backward2)
# Anova chi-square test to check the overall effect of variables
anova(backward2, test = "Chisq")
# Drop columns of diameter, upstream_invert and pipe_shape

model_str2 <- glm(formula = structural_condition ~ 
                       years + material + 
                       pipe_length + pipe_shape + 
                    downstream_invert + local_board, 
                     family = "binomial", 
                     data = rose_train_str)
summary(model_str2)

# Model prediction
pred2 <- predict(model_str2,test, type="response")
# Plot ROC curve
plotROC(test$structural_condition, pred2) #0.74
# Confusion Matrix
pred2 <- as.integer(pred2>0.5)
confusionMatrix(test$structural_condition, as.factor(pred2), positive = "1")
```
The sensitivity of ROSE train model (0.49) is much lower than that of undersampling train model (0.69). Since we are more interested in the pipes of poor condition, we will pick the undersampling train data to be used for the final logistic regression model to predict strudctural condition of pipes. 

### 2.1.3 Final model to predict structural condition of pipes

```{r}
final_str <- glm(structural_condition ~ years + material + 
                    downstream_depth + pipe_length + 
                    downstream_invert + local_board,
                  family = "binomial", 
                  data = under_train_str)
# Variable Importance
varImp(final_str)

# Transform the coefficients from log-odds to odds
exp(final_str$coefficients)

# Check the results in a table
final_str %>%
  tidy(exponentiate = T, conf.int = T) %>%
  mutate_if(is.numeric, ~round(.,3)) %>%
  kable(align = c("l", rep("c",6)))
```
The table shows:
- estimate is the odds ratio
- std.error is the standard error of the odds ratio
- statistic is the z-statistic
- p.value is the significance
- conf.low is the lower level of the 95% confidence interval for the odds ratios
- conf.high is the upper level of the 95% confidence interval for the odds ratios

### 2.1.4 Explore the predictors using dominance analysis

Dominance analysis is a method to evaluate the importance of each predictor in multiple regression models such as OLS, GLM and HLM. It's computationally intensive as it builds all subset models (2^6-1 models), 
For ordinary least squares regressions, the predictorsâ€™ additional contribution to a certain subset model is defined as the change in R2 when the predictor is added to the model. In logistic regressions, several analogues of R2 were proposed as measures of model fit, but only four were considered according to three criteria (Azen and Traxel, 2009). 

```{r}
pR2(final_str)
# McFadden (r2.m), Cox and Snell (r2.cs), Nagelkerke (r2.n), and Estrella (r2.e).
da.glm.fit()("names")
# perform dominance analysis
da_str <- dominanceAnalysis(final_str)
summary(da_str)
# print the results of showing the McFadden index
getFits(da_str, "r2.m")
```
The first row represents the raw values of each univariate model. The following rows show the additional contribution of each predictor added to the subset model. Also, if the additional 
```{r}
dominanceMatrix(da_str,
                type = "complete",
                fit.functions = "r2.m",
                ordered = TRUE)

plot(da_str, 
     which.graph = "conditional",
     fit.function = "r2.m")

# explore general dominance by using average method
averageContribution(da_str, fit.functions = "r2.m")
# plot the average contribution
plot(da_str, 
     which.graph = "general",
     fit.function = "r2.m")

```

```{r}
# Summary of SW dataset to get median
summary(SW)

## New data set to explore the predicted probability of pipes
## in poor structural condition by age in different materials
## in the area of Auckland Central with only the circle pipe shape.
newdata <- expand.grid(years = seq(0, 200, by=1),
  material = factor(1:6),
  local_board = factor(18), # Set local board area to Waitemata
  downstream_depth = 2.1,
  downstream_invert = 0,
  pipe_length = 34.43)
# predicted values using the final model
pred_prob <- augment(final_str,
                     type.predict = "response",
                     newdata = newdata,
                     se_fit = TRUE)
plotdata <- pred_prob %>%
  rename(predprob = .fitted,
         se = .se.fit)
plotdata %>% 
  ggplot(aes(x = years, y = predprob, fill = as.factor(material),
             color = as.factor(material))) +
  geom_line() +
  labs(title = "Predicted Probability of Pipes in Poor Structural Condition by Age and Material",
       color="Material",
       x = "Age",
       y = "Predicted Probability") +
  scale_color_discrete(labels = c("Other", "Asbestos Cement",
                                 "Concrete", "Earthenware",
                                 "Polyvinyl", "Polyvinyl Chloride")) +
  theme(plot.title = element_text(size = 10))
```

We get the predicted probabilities plotted across the range of ages with separate lines for pipe materials, holding the area in Auckland Central, the circular pipe shape, and median number of the rest numeric predictors. Based on the model, we can tell that the probability of pipes in poor structural condition increases in pipe age, but the material of Asbestos cement is the highest compared to others.

```{r}
## Another new data set to explore the predicted probability of pipes
## in poor structural condition by age in different location suburbs
## while keep other predictors using the median number

newdata2 <- expand.grid(years = seq(0,200,by=1),
  material = factor(4), # Set concrete pipe
  local_board = factor(c(3:6, 8, 9)), 
  downstream_depth = 2.1,
  downstream_invert = 0,
  pipe_length = 34.43)
# predicted values using the final model
pred_prob2 <- augment(final_str,
                     type.predict = "response",
                     newdata = newdata2,
                     se_fit = TRUE)
plotdata <- pred_prob2 %>%
  rename(predprob = .fitted,
         se = .se.fit)
plotdata %>% 
  ggplot(aes(x = years, y = predprob, fill = as.factor(local_board),
             color = as.factor(local_board))) +
  geom_line() +
  labs(title = "Predicted Probability of Pipes in Poor Structural Condition by Age and Pipe Locations",
       color="Local Board",
       x = "Age",
       y = "Predicted Probability") +
  scale_color_discrete(labels = c("Franklin", "Henderson-Massey",
                                 "Hibiscus and Bays", "Howick",
                                 "Mangere-Otahuhu", "Manurewa")) +
  theme(plot.title = element_text(size = 10))

# Albert-Eden(1), Devonport-Takapuna(2), Franklin(3), Henderson-Massey(4), Hibiscus and Bays(5), 
# Howick(6), Kaipatiki(7), Mangere-Otahuhu (8), Manurewa (9), Maungakiekie-Tamaki (10), 
# Orakei (11), Otara-Papatoetoe (12), Papakura (13), Puketapapa (14), Rodney (15), 
# Upper Harbour (16), Waitakere Ranges (17), Waitemata (18), Whau (19)
  
```

## 2.2 Predicting Service condition of Pipes

### 2.2.1 Over sampling train dataset

```{r}
# Logistic regression
full_ser <- glm(formula = service_condition ~ 
                   years + material + diameter +
                          pipe_length + downstream_depth +
                          upstream_depth + pipe_shape +
                          downstream_invert + upstream_invert +
                          oper_area + local_board, 
                 family = "binomial", data = over_train_ser)
summary(full_ser)

# Perform backward stepwise regression
backward3 <- step(full_ser, direction='backward', scope=formula(full_ser), trace=0)
summary(backward3)
# Likelihood Test
backward3$anova
# Anova chi-square test to check the overall effect of variables
anova(backward3, test = "Chisq")
# All factors are significant
model_ser <- glm(formula = service_condition ~ years + material + diameter + 
    pipe_length + downstream_depth + upstream_depth + pipe_shape + 
    downstream_invert + upstream_invert + local_board, family = "binomial", 
    data = over_train_ser)

## Model prediction
pred3 <- predict(model_ser,test, type="response")
# Plot ROC curve 
plotROC(test$service_condition, pred3) # 0.69
pred3 <- as.integer(pred3 > 0.5)
## Confusion Matrix
confusionMatrix(test$service_condition, as.factor(pred3), positive = "1")

#             Reference
# Prediction    0    1
#        0   3536 2161
#        1    416  747
```

### 2.2.2 ROSE train dataset
```{r}
full_ser2 <- glm(formula = service_condition ~ 
                   years + material + diameter +
                          pipe_length + downstream_depth +
                          upstream_depth + pipe_shape +
                          downstream_invert + upstream_invert +
                          oper_area + local_board, 
                 family = "binomial", data = rose_train_ser)
summary(full_ser2)

# Perform backward stepwise regression
backward4 <- step(full_ser2, direction='backward', scope=formula(full_ser2), trace=0)
summary(backward4)
# Likelihood Test
backward4$anova
# Anova chi-square test to check the overall effect of variables
anova(backward4, test = "Chisq")

# All factors are significant
model_ser2 <- glm(formula = service_condition ~ years + material + diameter + 
    pipe_length + downstream_depth + upstream_depth + pipe_shape + 
    downstream_invert + upstream_invert + local_board, family = "binomial", 
    data = rose_train_ser)

# check multicollinearity
car::vif(model_ser2)

## Model prediction
pred4 <- predict(model_ser2,test, type="response")
# Plot ROC curve 
plotROC(test$service_condition, pred4) # 0.68
pred4 <- as.integer(pred4 > 0.5)
## Confusion Matrix
confusionMatrix(test$service_condition, as.factor(pred4), positive = "1")

```

### 2.2.3 Final Model
```{r}
# Final model to predict service condition of pipes
final_ser <- glm(formula = service_condition ~ years + material + diameter +
    pipe_length + downstream_depth + upstream_depth + pipe_shape + 
    downstream_invert + upstream_invert + local_board, family = "binomial", 
    data = over_train_ser)

# Variable Importance
varImp(final_ser)

# Transform the coefficients from log-odds to odds
exp(final_ser$coefficients)

# Check the results in a table
final_ser %>%
  tidy(exponentiate = T, conf.int = T) %>%
  mutate_if(is.numeric, ~round(.,3)) %>%
  kable(align = c("l", rep("c",6)))
```

### 2.2.4 Deterioration Curve of Service Condition

```{r}
# Summary of SW dataset to get median
summary(SW)

## New data set to explore the predicted probability of pipes
## in poor structural condition by age in different materials
## in the area of Auckland Central with only the circle pipe shape.
newdata3 <- expand.grid(years = seq(0, 200, by=1),
  material = factor(1:6),
  local_board = factor(18), # Set local board area to Waitemata
  downstream_depth = 2.1,
  upstream_depth = 1.96,
  downstream_invert = 0,
  upstream_invert = 0,
  pipe_shape = factor(1),
  diameter = 450,
  pipe_length = 34.43)
# predicted values using the final model
pred_prob <- augment(final_ser,
                     type.predict = "response",
                     newdata = newdata3,
                     se_fit = TRUE)
plotdata <- pred_prob %>%
  rename(predprob = .fitted,
         se = .se.fit)
plotdata %>% 
  ggplot(aes(x = years, y = predprob, fill = as.factor(material),
             color = as.factor(material))) +
  geom_line() +
  labs(title = "Predicted Probability of Pipes in Poor Structural Condition by Age and Material",
       color="Material",
       x = "Age",
       y = "Predicted Probability") +
  scale_color_discrete(labels = c("Other", "Asbestos Cement",
                                 "Concrete", "Earthenware",
                                 "Polyvinyl", "Polyvinyl Chloride")) +
  theme(plot.title = element_text(size = 10))
```

# Decision Tree
```{r}
library(party)
tree <- ctree(structural_condition ~ years + material + 
                    downstream_depth + pipe_length + 
                    downstream_invert + local_board, 
              data = train,
              controls = ctree_control(mincriterion = 0.99, 
                                       minsplit = 1000))
tree
plot(tree)

# prediction
predict(tree, test, type = "prob")

tab <- table(predict(tree), train$structural_condition)
tab

1- sum(diag(tab))/sum(tab)

testPred <- predict(tree, newdata=test)
tab1 <- table(testPred, test$structural_condition)
tab1
1-sum(diag(tab1))/sum(tab1)

# Decision tree usint other packages
library(rpart)
library(rpart.plot)

fit <- rpart(structural_condition ~ years + material + 
                    downstream_depth + pipe_length + 
                    downstream_invert + local_board,
             data = train, 
             method = "class")
rpart.plot(fit, extra = 106)
predict_unseen <- predict(fit, test, type = "class")
table_mat <- table(test$structural_condition, 
                   predict_unseen)
table_mat
sum(diag(table_mat)) / sum(table_mat)
```


# C5.0 Decision Tree 
```{r}
library(C50)
library(highcharter)

learn_c50 <- C5.0(train[,1:11], train$structural_condition)
summary(learn_c50)
plot(learn_c50)
pre_c50 <- predict(learn_c50, test)
cm_c50 <- confusionMatrix(pre_c50, test$structural_condition, positive = "1")
cm_c50
# C5.0 Tune
acc_test <- numeric()
accuracy1 <- NULL; accuracy2 <- NULL

for(i in 1:50){
    learn_imp_c50 <- C5.0(train[,1:11], 
                          train$structural_condition,
                          trials = i)      
    p_c50 <- predict(learn_imp_c50, test) 
    accuracy1 <- confusionMatrix(p_c50, test$structural_condition)
    accuracy2[i] <- accuracy1$overall[1]
}

acc <- data.frame(t= seq(1,50), cnt = accuracy2)

opt_t <- subset(acc, cnt==max(cnt))[1,]
sub <- paste("Optimal number of trials is", opt_t$t, "(accuracy :", opt_t$cnt,") in C5.0")


hchart(acc, 'line', hcaes(t, cnt)) %>%
  hc_title(text = "Accuracy With Varying Trials (C5.0)") %>%
  hc_subtitle(text = sub) %>%
  hc_add_theme(hc_theme_google()) %>%
  hc_xAxis(title = list(text = "Number of Trials")) %>%
  hc_yAxis(title = list(text = "Accuracy"))


# Apply optimal trials to show best predict performance in C5.0
learn_imp_c50 <- C5.0(train[,1:11],
                      train$structural_condition,
                      trials=opt_t$t)    
pre_imp_c50 <- predict(learn_imp_c50, test)
cm_imp_c50 <- confusionMatrix(pre_imp_c50, 
                              test$structural_condition,
                              positive = "1")
cm_imp_c50

```

# Random Forest

```{r}
set.seed(123)
library(randomForest)
learn_rf <- randomForest(structural_condition ~ years + material + 
                    downstream_depth + pipe_length + 
                    downstream_invert + local_board,
             data = train, 
             ntree=500, 
             proximity=T, 
             importance=T)
pre_rf   <- predict(learn_rf, test)
cm_rf    <- confusionMatrix(pre_rf, test$structural_condition)
cm_rf
```
# Supoprt Vector Machine
```{r}
library(e1071)
library(caret)
learn_svm <- svm(structural_condition ~ years + material + 
                    downstream_depth + pipe_length + 
                    downstream_invert + local_board,
             data = train)
pre_svm <- predict(learn_svm, test)
cm_svm <- confusionMatrix(pre_svm, test$structural_condition, positive = "1")
cm_svm
```

# SVM Tune
```{r}
gamma <- seq(0,0.1,0.005)
cost <- 2^(0:5)
parms <- expand.grid(cost=cost, gamma=gamma)  

acc_test <- numeric()
accuracy1 <- NULL; accuracy2 <- NULL

for(i in 1:NROW(parms)){        
        learn_svm <- svm(structural_condition ~ years + material + 
                    downstream_depth + pipe_length + 
                    downstream_invert + local_board,
                    data = train,
                    gamma=parms$gamma[i],
                    cost=parms$cost[i])
        pre_svm <- predict(learn_svm, test)
        accuracy1 <- confusionMatrix(pre_svm, test$structural_condition)
        accuracy2[i] <- accuracy1$overall[1]
}

acc <- data.frame(p= seq(1,NROW(parms)), cnt = accuracy2)

opt_p <- subset(acc, cnt==max(cnt))[1,]
sub <- paste("Optimal number of parameter is", opt_p$p, "(accuracy :", opt_p$cnt,") in SVM")

library(highcharter)
hchart(acc, 'line', hcaes(p, cnt)) %>%
  hc_title(text = "Accuracy With Varying Parameters (SVM)") %>%
  hc_subtitle(text = sub) %>%
  hc_add_theme(hc_theme_google()) %>%
  hc_xAxis(title = list(text = "Number of Parameters")) %>%
  hc_yAxis(title = list(text = "Accuracy"))
```


